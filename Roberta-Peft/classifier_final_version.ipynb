{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install matplotlib scikit-learn\n",
        "!pip install peft"
      ],
      "metadata": {
        "id": "AaZEZzETOqxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Am3Zj0GQJKs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import RobertaForSequenceClassification,AdamW\n",
        "from transformers import RobertaTokenizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import RocCurveDisplay\n"
      ],
      "metadata": {
        "id": "ImgMRBkWOxvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "wJcjd1f9KcNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model_trainer:\n",
        "  def __init__(self,model,tokenizer,dataset,isPeft=False):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.dataset = dataset\n",
        "    self.isPeft = isPeft\n",
        "    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    self.epoch=3\n",
        "    self.isTrain=False\n",
        "\n",
        "  def preprocess_data(self):\n",
        "    df = pd.read_csv(self.dataset)\n",
        "    df['combined_text'] = df['x_i'] + ' ' + df['x_j']\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(df['combined_text'], df['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "  def tokenize_data(self):\n",
        "    self.X_train = [str(text) for text in self.X_train]\n",
        "    self.X_test = [str(text) for text in self.X_test]\n",
        "    self.train_encodings = tokenizer(list(self.X_train), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "    self.test_encodings = tokenizer(list(self.X_test), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "  def create_dataloaders(self,classObj):\n",
        "    self.y_train = self.y_train.astype('category').cat.codes.tolist()\n",
        "    self.y_test = self.y_test.astype('category').cat.codes.tolist()\n",
        "    train_dataset = classObj(self.train_encodings, self.y_train)\n",
        "    test_dataset = classObj(self.test_encodings, self.y_test)\n",
        "    self.train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    self.test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "  def lora(self):\n",
        "    config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"classifier\"],\n",
        "    )\n",
        "    self.model = get_peft_model(self.model, config)\n",
        "\n",
        "  def train(self,classObj):\n",
        "    self.isTrain=True\n",
        "    if self.isPeft:\n",
        "      self.lora()\n",
        "    self.model.to(self.device)\n",
        "    optimizer = AdamW(self.model.parameters(), lr=5e-5)\n",
        "    self.model.train()\n",
        "    self.preprocess_data()\n",
        "    self.tokenize_data()\n",
        "    self.create_dataloaders(classObj)\n",
        "    for epoch in range(self.epoch):\n",
        "        loop = tqdm(self.train_loader, leave=True)\n",
        "        for batch in loop:\n",
        "          batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "          outputs = self.model(**batch)\n",
        "          loss = outputs.loss\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          loop.set_description(f'Epoch {epoch}')\n",
        "          loop.set_postfix(loss=loss.item())\n",
        "  def get_metrics(self):\n",
        "    if not self.isTrain:\n",
        "      raise Exception(\"Model is not trained\")\n",
        "    self.model.eval()\n",
        "    self.preds = []\n",
        "    self.true_labels = []\n",
        "    class_names = ['GPT2', 'GPT4o', 'GPT_NEO', 'Gemini', 'Reformer']\n",
        "    with torch.no_grad():\n",
        "        for batch in self.test_loader:\n",
        "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "            outputs = self.model(**batch)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            labels = batch['labels'].cpu().numpy()\n",
        "            self.preds.extend(predictions)\n",
        "            self.true_labels.extend(labels)\n",
        "\n",
        "    accuracy = accuracy_score(self.true_labels, self.preds)\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(classification_report(self.true_labels, self.preds, target_names=class_names))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "  def get_samples(self):\n",
        "    if not self.isTrain:\n",
        "      raise Exception(\"Model is not trained\")\n",
        "    class_names = ['GPT2', 'GPT4o', 'GPT_NEO', 'Gemini', 'Reformer']\n",
        "    random_indices = random.sample(range(len(self.X_test)), 10)\n",
        "    for idx in random_indices:\n",
        "      text = self.X_test[idx]\n",
        "      true_label = class_names[self.true_labels[idx]]\n",
        "      predicted_label = class_names[self.preds[idx]]\n",
        "      print(f\"Text: {text}\")\n",
        "      print(f\"True Label: {true_label}\")\n",
        "      print(f\"Predicted Label: {predicted_label}\")\n",
        "      print(f\"Match: {true_label == predicted_label}\")\n",
        "      print('-' * 80)\n",
        "\n",
        "  def auc_roc(self):\n",
        "    if not self.isTrain:\n",
        "      raise Exception(\"Model is not trained\")\n",
        "    class_names = ['GPT2', 'GPT4o', 'GPT_NEO', 'Gemini', 'Reformer']\n",
        "    y_true_binarized = label_binarize(self.true_labels, classes=[0, 1, 2, 3, 4])\n",
        "    y_pred_binarized = label_binarize(self.preds, classes=[0, 1, 2, 3, 4])\n",
        "    n_classes = len(class_names)\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(n_classes):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_binarized[:, i])\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i in range(n_classes):\n",
        "      plt.plot(fpr[i], tpr[i], label=f'ROC curve for {class_names[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6FdrzY4LI15W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
        "trainer=model_trainer(model,tokenizer,\"LLM_dataset.csv\",True)"
      ],
      "metadata": {
        "id": "0I29_j4NOzU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(TextDataset)"
      ],
      "metadata": {
        "id": "065SnQk-QjoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.get_metrics()"
      ],
      "metadata": {
        "id": "O0dO09W2SNBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.get_samples()"
      ],
      "metadata": {
        "id": "YmCcztvmSSxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.auc_roc()"
      ],
      "metadata": {
        "id": "A4hrv9xlSV18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}