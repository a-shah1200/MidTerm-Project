{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "pJlUTseDWvT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "859CtFnmxmzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate"
      ],
      "metadata": {
        "id": "M16wD8OKxqzX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import os"
      ],
      "metadata": {
        "id": "PS9raMHZxcn3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model_trainer:\n",
        "  def  __init__(self,name,dataset,isPEFT=False):\n",
        "    self.name=name\n",
        "    self.isPEFT=isPEFT\n",
        "    self.dataset=dataset\n",
        "    self.id2label = {0: \"GPT2\", 1: \"GPT4o\",2:\"GPT_NEO\",3:\"Gemini\",4:\"Reformer\"}\n",
        "    self.label2id = {d:i for i, d in self.id2label.items()}\n",
        "    self.prep_data()\n",
        "\n",
        "  def prep_data(self):\n",
        "    df=pd.read_csv(self.dataset)\n",
        "    df.fillna(\" \",inplace=True)\n",
        "    df[\"x_i+x_j\"]=df[\"x_i\"]+[\" \"]+df[\"x_j\"]\n",
        "    df[\"Label\"]=df[\"Label\"].map(self.label2id)\n",
        "    df.drop([\"x_i\",\"x_j\"],axis=1,inplace=True)\n",
        "    df.to_csv(\"LLM_dataset_comb.csv\",index=False)\n",
        "\n",
        "  def dataset_train(self):\n",
        "    dataset = load_dataset(\"csv\", data_files=\"LLM_dataset_comb.csv\")\n",
        "    dataset = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.name)\n",
        "    def preprocess_function(examples):\n",
        "      return self.tokenizer(examples[\"x_i+x_j\"], truncation=True, padding='max_length', max_length=512)\n",
        "    tokenized_data = dataset.map(preprocess_function)\n",
        "    tokenized_data=tokenized_data.rename_column(\"Label\", \"label\")\n",
        "    tokenized_data=tokenized_data.rename_column(\"x_i+x_j\", \"text\")\n",
        "    return tokenized_data\n",
        "\n",
        "  def prep_model(self):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(self.name, num_labels=5, id2label=self.id2label, label2id=self.label2id)\n",
        "    if self.isPEFT:\n",
        "\n",
        "      raise NotImplementedError(\"Error 1\")\n",
        "    else:\n",
        "      for name,param in model.named_parameters():\n",
        "        if name.startswith(\"distilbert\"):\n",
        "          param.requires_grad=False\n",
        "    return model\n",
        "\n",
        "  def train(self):\n",
        "    tokenized_data=self.dataset_train()\n",
        "    model=self.prep_model()\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "      predictions, labels = eval_pred\n",
        "      predictions = np.argmax(predictions, axis=1)\n",
        "      return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    try:\n",
        "      os.mkdir(\"model_saved\")\n",
        "    except:\n",
        "      pass\n",
        "    training_args = TrainingArguments(\n",
        "    output_dir=\"model_saved\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16, # Batch Size\n",
        "    num_train_epochs=2, # Total Epcohs\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit = 2)\n",
        "\n",
        "    trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=tokenized_data[\"train\"],\n",
        "      eval_dataset=tokenized_data[\"test\"],\n",
        "      tokenizer=self.tokenizer,\n",
        "      compute_metrics=compute_metrics )\n",
        "    trainer.train()\n",
        "    return trainer.state.best_model_checkpoint # Returns best checkpoint of model\n"
      ],
      "metadata": {
        "id": "2tszV8UmtetK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name=\"distilbert/distilbert-base-uncased\"\n",
        "dataset=\"LLM_dataset.csv\""
      ],
      "metadata": {
        "id": "BGdK1P0VyQLi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer(name,dataset).train()"
      ],
      "metadata": {
        "id": "aQqWVlbcywxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}